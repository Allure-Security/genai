{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chain-of-Thought (CoT)\n",
    "\n",
    "LLMs can be useful for a variety of tasks. Sometimes though, they need a little nudging in the right direction to get the desired outcome. Broadly, reasoning refers to a collection of techniques that can perform this nudge. In essence, we want to make the model \"think\" through things, break a task down into individual steps or explain its own reasoning.\n",
    "\n",
    "Note: When we say reason or think in this context, we are not referring to the kind of thinking or reasoning that we, as humans, are capable of. This [blog](https://cacm.acm.org/blogs/blog-cacm/276268-can-llms-really-reason-and-plan/fulltext) encapsulates the difference between these two ideas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load environment variables\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\"../../.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools import llm_call"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Failure Case\n",
    "\n",
    "The typical CoT use case is with reasoning or numerical tasks like the example below. We offer a fairly simple task. Add up the number in a list and decide if the preceding statement is true or false.\n",
    "\n",
    "Note: OpenAI updates GPT fairly regularly so these examples go out of date sometimes. So you may not get the same answer when you try this out later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------PROMPT----------\n",
      "\n",
      "Answer with True or False:\n",
      "The numbers in the following list add up to an even number: [0, 1, 2, 42, 37, 59, 12]\n",
      "\n",
      "---------RESPONSE---------\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Answer with True or False:\n",
    "The numbers in the following list add up to an even number: [0, 1, 2, 42, 37, 59, 12]\n",
    "\"\"\"\n",
    "\n",
    "print(\"----------PROMPT----------\")\n",
    "print(prompt)\n",
    "print(\"---------RESPONSE---------\")\n",
    "result = llm_call(model=\"gpt-4\", prompt=prompt, parameters={\"temperature\": 0.0})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153\n"
     ]
    }
   ],
   "source": [
    "print(sum([0, 1, 2, 42, 37, 59, 12]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding the same list up, we see that the answer should actually be false! Now let's try this with Chain-of-Thought."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-Shot CoT\n",
    "\n",
    "Like Zero-Shot and Few-Shot prompting, we can also perform CoT in a similar manner. For the Zero-Shot case, it's simply a matter of appending a phrase like \"Let's think step by step\" to the end of the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------PROMPT----------\n",
      "\n",
      "Answer with True or False:\n",
      "The numbers in the following list add up to an even number: [0, 1, 2, 42, 37, 59, 12]\n",
      "Let's think step by step.\n",
      "\n",
      "---------RESPONSE---------\n",
      "First, let's add up the numbers: 0 + 1 + 2 + 42 + 37 + 59 + 12 = 153\n",
      "\n",
      "153 is an odd number, not an even number.\n",
      "\n",
      "So, the statement is False.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Answer with True or False:\n",
    "The numbers in the following list add up to an even number: [0, 1, 2, 42, 37, 59, 12]\n",
    "Let's think step by step.\n",
    "\"\"\"\n",
    "\n",
    "print(\"----------PROMPT----------\")\n",
    "print(prompt)\n",
    "print(\"---------RESPONSE---------\")\n",
    "result = llm_call(model=\"gpt-4\", prompt=prompt, parameters={\"temperature\": 0.0})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a slight change to the prompt and no changes to the model or parameters, we get the right answer. By simply \"asking\" the LLM to think through this step by step, we get the right answer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few-Shot CoT\n",
    "\n",
    "In other, more complex scenarios, the LLM might not be able to get the right answer even with zero-shot CoT. In such cases, examples are really helpful. Few-Shot CoT is pretty straightforward. We do the same as above, except we inject a few examples for the model to learn from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------PROMPT----------\n",
      "\n",
      "Answer with True or False:\n",
      "The numbers in the following list add up to an even number: [21, 8, 11, 2]\n",
      "Let's think step by step.\n",
      "The sum of all numbers in the list is 42. 42 is even. The statement is True.\n",
      "\n",
      "Answer with True or False:\n",
      "The numbers in the following list add up to an odd number: [0, 3, 2, 3, 4, 9, 1]\n",
      "Let's think step by step.\n",
      "The sum of all numbers in the list is 22. 22 is even. The statement is False.\n",
      "\n",
      "Answer with True or False:\n",
      "The numbers in the following list add up to an even number: [0, 1, 2, 42, 37, 59, 12]\n",
      "Let's think step by step.\n",
      "\n",
      "---------RESPONSE---------\n",
      "The sum of all numbers in the list is 153. 153 is odd. The statement is False.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Answer with True or False:\n",
    "The numbers in the following list add up to an even number: [21, 8, 11, 2]\n",
    "Let's think step by step.\n",
    "The sum of all numbers in the list is 42. 42 is even. The statement is True.\n",
    "\n",
    "Answer with True or False:\n",
    "The numbers in the following list add up to an odd number: [0, 3, 2, 3, 4, 9, 1]\n",
    "Let's think step by step.\n",
    "The sum of all numbers in the list is 22. 22 is even. The statement is False.\n",
    "\n",
    "Answer with True or False:\n",
    "The numbers in the following list add up to an even number: [0, 1, 2, 42, 37, 59, 12]\n",
    "Let's think step by step.\n",
    "\"\"\"\n",
    "\n",
    "print(\"----------PROMPT----------\")\n",
    "print(prompt)\n",
    "print(\"---------RESPONSE---------\")\n",
    "result = llm_call(model=\"gpt-4\", prompt=prompt, parameters={\"temperature\": 0.0})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that the model learns how to solve the problem from the example and also gives us answers using the same style of reasoning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Consistency\n",
    "\n",
    "So from the above examples we know that the LLM can at times give us the wrong answer. But we can't always verify the answers manually. One interesting approach to getting the right answer is to simply ask the LLM the same question multiple times. More often than not, the LLM ends up giving the right answer the majority of the time. In this scenario, we could then just take the most popular answer as the right answer. When combined with CoT techniques from above and tools that we'll discuss over the next two days, a more robust system can be established.\n",
    "\n",
    "Now there is one obvious caveat here - this is expensive. We're increasing our costs N-fold where N is the number of times we query the model for a given prompt. Thus while self-consistency works to some extent, it is not very scalable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------PROMPT----------\n",
      "\n",
      "Answer with True or False:\n",
      "The numbers in the following list add up to an even number: [0, 1, 2, 42, 37, 59, 12]\n",
      "Answer: \n",
      "\n",
      "---------RESPONSE---------\n",
      "['True', 'False', 'False', 'True', 'False']\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Answer with True or False:\n",
    "The numbers in the following list add up to an even number: [0, 1, 2, 42, 37, 59, 12]\n",
    "Answer: \n",
    "\"\"\"\n",
    "\n",
    "print(\"----------PROMPT----------\")\n",
    "print(prompt)\n",
    "print(\"---------RESPONSE---------\")\n",
    "result = [llm_call(model=\"gpt-4\", prompt=prompt, parameters={\"temperature\": 1.0}) for i in range(5)]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that we got the right answer (False) more than the wrong answer. Thus if we took the most frequent response here, we would get the answer we needed. \n",
    "\n",
    "Note: We've increased the temperature of the model from 0 to 1 here as setting a temperature of 0 makes the model more likely (no guarantees with GPT-4!) to give the same answer on each run. The temperature parameter is something we would recommend tuning based on your use case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Least-to-Most Prompting\n",
    "\n",
    "Least-to-Most prompting is a way to get the model to break down tasks into a set of subtasks. Then each subtask is solved in order with the prompt and solution for the previous subtask being fed into the model at each step. In essence, we want to solve the smallest individual problem and slowly build our way to the final answer. The example below demonstrates this two step process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------PROMPT----------\n",
      "\n",
      "What subproblems must be solved before solving the following task? Each subproblem must be a new task.\n",
      "```\n",
      "Answer with True or False:\n",
      "The numbers in the following list add up to an even number: [0, 1, 2, 42, 37, 59, 12]\n",
      "```\n",
      "\n",
      "---------RESPONSE---------\n",
      "1. Identify the numbers in the list.\n",
      "2. Add all the numbers in the list together.\n",
      "3. Determine if the sum is an even number.\n",
      "4. Formulate a response in a True or False format.\n"
     ]
    }
   ],
   "source": [
    "problem = \"\"\"Answer with True or False:\n",
    "The numbers in the following list add up to an even number: [0, 1, 2, 42, 37, 59, 12]\"\"\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "What subproblems must be solved before solving the following task? Each subproblem must be a new task.\n",
    "```\n",
    "{problem}\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "print(\"----------PROMPT----------\")\n",
    "print(prompt)\n",
    "print(\"---------RESPONSE---------\")\n",
    "result = llm_call(model=\"gpt-4\", prompt=prompt, parameters={\"temperature\": 0.0})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can take each of these tasks and solve them one by one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------PROMPT----------\n",
      "\n",
      "Given the following context and a list of tasks, solve any tasks that do not already have a solution.\n",
      "Context:\n",
      "```\n",
      "Answer with True or False:\n",
      "The numbers in the following list add up to an even number: [0, 1, 2, 42, 37, 59, 12]\n",
      "```\n",
      "Tasks:\n",
      "\n",
      "1. Identify the numbers in the list.\n",
      "---------RESPONSE---------\n",
      "Solution: The numbers in the list are 0, 1, 2, 42, 37, 59, 12.\n",
      "##########################################################################################\n",
      "----------PROMPT----------\n",
      "\n",
      "Given the following context and a list of tasks, solve any tasks that do not already have a solution.\n",
      "Context:\n",
      "```\n",
      "Answer with True or False:\n",
      "The numbers in the following list add up to an even number: [0, 1, 2, 42, 37, 59, 12]\n",
      "```\n",
      "Tasks:\n",
      "\n",
      "1. Identify the numbers in the list.\n",
      "Solution: The numbers in the list are 0, 1, 2, 42, 37, 59, 12.\n",
      "2. Add all the numbers in the list together.\n",
      "---------RESPONSE---------\n",
      "Solution: The sum of all the numbers in the list is 153.\n",
      "##########################################################################################\n",
      "----------PROMPT----------\n",
      "\n",
      "Given the following context and a list of tasks, solve any tasks that do not already have a solution.\n",
      "Context:\n",
      "```\n",
      "Answer with True or False:\n",
      "The numbers in the following list add up to an even number: [0, 1, 2, 42, 37, 59, 12]\n",
      "```\n",
      "Tasks:\n",
      "\n",
      "1. Identify the numbers in the list.\n",
      "Solution: The numbers in the list are 0, 1, 2, 42, 37, 59, 12.\n",
      "2. Add all the numbers in the list together.\n",
      "Solution: The sum of all the numbers in the list is 153.\n",
      "3. Determine if the sum is an even number.\n",
      "---------RESPONSE---------\n",
      "Solution: The sum of the numbers, 153, is not an even number. So, the answer is False.\n",
      "##########################################################################################\n",
      "----------PROMPT----------\n",
      "\n",
      "Given the following context and a list of tasks, solve any tasks that do not already have a solution.\n",
      "Context:\n",
      "```\n",
      "Answer with True or False:\n",
      "The numbers in the following list add up to an even number: [0, 1, 2, 42, 37, 59, 12]\n",
      "```\n",
      "Tasks:\n",
      "\n",
      "1. Identify the numbers in the list.\n",
      "Solution: The numbers in the list are 0, 1, 2, 42, 37, 59, 12.\n",
      "2. Add all the numbers in the list together.\n",
      "Solution: The sum of all the numbers in the list is 153.\n",
      "3. Determine if the sum is an even number.\n",
      "Solution: The sum of the numbers, 153, is not an even number. So, the answer is False.\n",
      "4. Formulate a response in a True or False format.\n",
      "---------RESPONSE---------\n",
      "Solution: False\n",
      "##########################################################################################\n"
     ]
    }
   ],
   "source": [
    "new_prompt = f\"\"\"\n",
    "Given the following context and a list of tasks, solve any tasks that do not already have a solution.\n",
    "Context:\n",
    "```\n",
    "{problem}\n",
    "```\n",
    "Tasks:\n",
    "\"\"\"\n",
    "\n",
    "tasks = result.split(\"\\n\")\n",
    "\n",
    "for task in tasks:\n",
    "    new_prompt += \"\\n\" + task\n",
    "    print(\"----------PROMPT----------\")\n",
    "    print(new_prompt)\n",
    "    print(\"---------RESPONSE---------\")\n",
    "    result = llm_call(model=\"gpt-4\", prompt=new_prompt, parameters={\"temperature\": 0.0})\n",
    "    print(result)\n",
    "    new_prompt += \"\\n\" + result\n",
    "    print(\"###\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this took quite a few steps and quite a few calls to the model, we've managed to break down the problem into its constituent steps and then solve each step one at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taking Reasoning Further\n",
    "\n",
    "This notebook has shown you some popular approaches towards reasoning. Tomorrow, we'll talk about tools and agents which can be used to extend the functionality of LLMs. When used in conjunction with reasoning techniques, we obtain powerful frameworks like ReAct and Plan-and-Solve which can reason on a problem, determine next steps, use tools such as Google searches or Python CLIs, and more!\n",
    "\n",
    "You can read more about this and other concepts in the [Georgian AI Library](https://github.com/georgian-io/GAL/blob/main/Prompting%20Tools%20%26%20Techniques/reasoning.md)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai-bootcamp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
