{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 1 - Prompting Basics\n",
    "\n",
    "In this notebook we cover the basic ideas of prompting including zero-shot prompting, few-shot prompting, prompt chaining, system prompts and memory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load environment variables\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\"../../.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools import llm_call"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context Length/Size\n",
    "\n",
    "Context length refers to the maximum number of tokens we can feed as input to a model. Roughly speaking 1 token = 3/4 of a word. We list the context sizes for the models we use below:\n",
    "\n",
    "* `gpt-4`: 8192 tokens\n",
    "* `gpt-3.5-turbo`: 4097 tokens\n",
    "* `text-bison`: 8192 tokens\n",
    "* `chat-bison`: 8192 tokens\n",
    "* `llama-2`: 4096 tokens\n",
    "\n",
    "Note that in general the entirety of the input you send to the model counts towards this context size. This includes the prompt, the system prompt (explained below) and the chat history.\n",
    "\n",
    "You can find more information about OpenAI's models [here](https://platform.openai.com/docs/models/overview), Google's models [here](https://cloud.google.com/vertex-ai/docs/generative-ai/learn/models), Anthropic's models [here](https://docs.anthropic.com/claude/reference/selecting-a-model) and Meta's models [here](https://ai.meta.com/llama/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompting\n",
    "\n",
    "Prompting is something many people are familiar with already. You send in some plain text to the model and you get back an output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------PROMPT----------\n",
      "Who played Saruman in the Lord of the Rings trilogy?\n",
      "---------RESPONSE---------\n",
      "Christopher Lee played Saruman in the Lord of the Rings trilogy.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Who played Saruman in the Lord of the Rings trilogy?\"\n",
    "\n",
    "print(\"----------PROMPT----------\")\n",
    "print(prompt)\n",
    "print(\"---------RESPONSE---------\")\n",
    "print(llm_call(model=\"gpt-4\", prompt=prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-Shot Prompting\n",
    "\n",
    "Zero-shot prompting refers to a method of prompting LLMs to perform a task by directly asking it to do so, with no examples in the prompt. To demonstrate this, let's consider a more specific use case - say classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------PROMPT----------\n",
      "Classify the sentiment of the following review as \"positive\", \"neutral\" or \"negative\".\n",
      "\n",
      "Review: I loved the new Spider-Man movie!! The animation was really fluid\n",
      "Sentiment: \n",
      "\n",
      "---------RESPONSE---------\n",
      "Positive\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"Classify the sentiment of the following review as \"positive\", \"neutral\" or \"negative\".\n",
    "\n",
    "Review: I loved the new Spider-Man movie!! The animation was really fluid\n",
    "Sentiment: \n",
    "\"\"\"\n",
    "\n",
    "print(\"----------PROMPT----------\")\n",
    "print(prompt)\n",
    "print(\"---------RESPONSE---------\")\n",
    "print(llm_call(model=\"gpt-4\", prompt=prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few-Shot Prompting\n",
    "\n",
    "Few-shot prompting takes zero-shot prompting and adds a couple of examples for the model to learn from. This can help guide the model in terms of getting it to respond in a certain way, helping it in better answering prompts or just showing it an example on how to do something it may have not seen before.\n",
    "\n",
    "As a trivial example, the response above returns \"Positive\" instead of \"positive\" even though we explicitly ask for the latter. With few-shot prompting, we can fix that!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------PROMPT----------\n",
      "Classify the sentiment of the following review as \"positive\", \"neutral\" or \"negative\".\n",
      "\n",
      "Review: I didn't think the new Spider-Man movie was that great. It was a decent watch, had nice animation.\n",
      "Sentiment: neutral\n",
      "\n",
      "Review: The new Spider-Man movie was boring. I just can't watch animated movies..\n",
      "Sentiment: negative\n",
      "\n",
      "Review: I loved the new Spider-Man movie!! The animation was really fluid\n",
      "Sentiment: \n",
      "\n",
      "---------RESPONSE---------\n",
      "positive\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"Classify the sentiment of the following review as \"positive\", \"neutral\" or \"negative\".\n",
    "\n",
    "Review: I didn't think the new Spider-Man movie was that great. It was a decent watch, had nice animation.\n",
    "Sentiment: neutral\n",
    "\n",
    "Review: The new Spider-Man movie was boring. I just can't watch animated movies..\n",
    "Sentiment: negative\n",
    "\n",
    "Review: I loved the new Spider-Man movie!! The animation was really fluid\n",
    "Sentiment: \n",
    "\"\"\"\n",
    "\n",
    "print(\"----------PROMPT----------\")\n",
    "print(prompt)\n",
    "print(\"---------RESPONSE---------\")\n",
    "print(llm_call(model=\"gpt-4\", prompt=prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Chaining\n",
    "\n",
    "Sometimes one prompt is not enough. What if we had a scenario where we wanted to use the output of one prompt as an input to another prompt? We can chain them together! This concept is the basis for the idea of \"chains\" popularized by LangChain (which we'll cover tomorrow). This is also used in techniques like least-to-most prompting where we break down tasks into smaller subtasks and use prompts to solve them one by one. \n",
    "\n",
    "Consider the following example where we first task the model to extract information from a given document and then query the model based on the extracted information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph = \"\"\"Pizza (English: /ˈpiːtsə/ PEET-sə, Italian: [ˈpittsa], Neapolitan: [ˈpittsə]) is a dish of Italian origin consisting of a usually round, flat base of leavened wheat-based dough topped with tomatoes, cheese, and often various other ingredients (such as various types of sausage, anchovies, mushrooms, onions, olives, vegetables, meat, ham, etc.), which is then baked at a high temperature, traditionally in a wood-fired oven.[1]\n",
    "\n",
    "The term pizza was first recorded in the 10th century in a Latin manuscript from the Southern Italian town of Gaeta in Lazio, on the border with Campania.[2] Raffaele Esposito is often credited for creating modern pizza in Naples.[3][4][5][6] In 2009, Neapolitan pizza was registered with the European Union as a Traditional Speciality Guaranteed dish. In 2017, the art of making Neapolitan pizza was added to UNESCO's list of intangible cultural heritage.[7]\n",
    "\n",
    "Pizza and its variants are among the most popular foods in the world. Pizza is sold at a variety of restaurants, including pizzerias (pizza specialty restaurants), Mediterranean restaurants, via delivery, and as street food.[8] In Italy, pizza served in a restaurant is presented unsliced, and is eaten with the use of a knife and fork.[9][10] In casual settings, however, it is cut into wedges to be eaten while held in the hand. Pizza is also sold in grocery stores in a variety of forms, including frozen or as kits for self-assembly. They are then cooked using a home oven.\n",
    "\n",
    "In 2017, the world pizza market was US$128 billion, and in the US it was $44 billion spread over 76,000 pizzerias.[11] Overall, 13% of the U.S. population aged two years and over consumed pizza on any given day.[12]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------PROMPT----------\n",
      "List out all the ingredients used to create the dish in the following paragraph. Do not include ingredients that are not in the paragraph.\n",
      "Paragraph: Pizza (English: /ˈpiːtsə/ PEET-sə, Italian: [ˈpittsa], Neapolitan: [ˈpittsə]) is a dish of Italian origin consisting of a usually round, flat base of leavened wheat-based dough topped with tomatoes, cheese, and often various other ingredients (such as various types of sausage, anchovies, mushrooms, onions, olives, vegetables, meat, ham, etc.), which is then baked at a high temperature, traditionally in a wood-fired oven.[1]\n",
      "\n",
      "The term pizza was first recorded in the 10th century in a Latin manuscript from the Southern Italian town of Gaeta in Lazio, on the border with Campania.[2] Raffaele Esposito is often credited for creating modern pizza in Naples.[3][4][5][6] In 2009, Neapolitan pizza was registered with the European Union as a Traditional Speciality Guaranteed dish. In 2017, the art of making Neapolitan pizza was added to UNESCO's list of intangible cultural heritage.[7]\n",
      "\n",
      "Pizza and its variants are among the most popular foods in the world. Pizza is sold at a variety of restaurants, including pizzerias (pizza specialty restaurants), Mediterranean restaurants, via delivery, and as street food.[8] In Italy, pizza served in a restaurant is presented unsliced, and is eaten with the use of a knife and fork.[9][10] In casual settings, however, it is cut into wedges to be eaten while held in the hand. Pizza is also sold in grocery stores in a variety of forms, including frozen or as kits for self-assembly. They are then cooked using a home oven.\n",
      "\n",
      "In 2017, the world pizza market was US$128 billion, and in the US it was $44 billion spread over 76,000 pizzerias.[11] Overall, 13% of the U.S. population aged two years and over consumed pizza on any given day.[12]\n",
      "---------RESPONSE---------\n",
      "- Wheat-based dough\n",
      "- Tomatoes\n",
      "- Cheese\n",
      "- Sausage\n",
      "- Anchovies\n",
      "- Mushrooms\n",
      "- Onions\n",
      "- Olives\n",
      "- Vegetables\n",
      "- Meat\n",
      "- Ham\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"List out all the ingredients used to create the dish in the following paragraph. Do not include ingredients that are not in the paragraph.\\nParagraph: {paragraph}\"\n",
    "\n",
    "print(\"----------PROMPT----------\")\n",
    "print(prompt)\n",
    "print(\"---------RESPONSE---------\")\n",
    "output = llm_call(model=\"gpt-4\", prompt=prompt)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now chain the result of the previous prompt into our question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------PROMPT----------\n",
      "From the following list of ingredients, classify them into vegetarian and non-vegetarian options. \n",
      "Ingredients: \n",
      "- Wheat-based dough\n",
      "- Tomatoes\n",
      "- Cheese\n",
      "- Sausage\n",
      "- Anchovies\n",
      "- Mushrooms\n",
      "- Onions\n",
      "- Olives\n",
      "- Vegetables\n",
      "- Meat\n",
      "- Ham\n",
      "---------RESPONSE---------\n",
      "Vegetarian:\n",
      "- Wheat-based dough\n",
      "- Tomatoes\n",
      "- Cheese\n",
      "- Mushrooms\n",
      "- Onions\n",
      "- Olives\n",
      "- Vegetables\n",
      "\n",
      "Non-Vegetarian:\n",
      "- Sausage\n",
      "- Anchovies\n",
      "- Meat\n",
      "- Ham\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"From the following list of ingredients, classify them into vegetarian and non-vegetarian options. \\nIngredients: \\n{output}\"\n",
    "\n",
    "print(\"----------PROMPT----------\")\n",
    "print(prompt)\n",
    "print(\"---------RESPONSE---------\")\n",
    "print(llm_call(model=\"gpt-4\", prompt=prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the above is a pretty simple example, we can imagine other cases where this would be useful. For example, let's say we have a 20 page document we want to summarize. It might not fit into the model entirely. Instead we could summarize each page of the document and then concatenate all the summaries to get a short version of the document. We can then summarize this new document to get a final summary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System Prompts\n",
    "\n",
    "A system prompt is a prompt we can use to set the \"behavior\" of the LLM for the duration of the conversation. Typical use cases include giving the model a set of instructions to follow during all future interactions in the chat session. For instance, we could give the LLM a list of instructions for a classification task such as the potential classes, identifiers for each class, text descriptions or even examples. Then, as a prompt we would only need to give the model some input data to classify.\n",
    "\n",
    "In general, the system prompt only exists in Chat models which are designed for conversations. Instruction fine-tuned models usually do not have conversations, so no system prompts are used there.\n",
    "\n",
    "In the following example we consider the same prompt twice - once without a system prompt and once with it.\n",
    "\n",
    "Note: Claude does not pay a lot of attention to the system prompt, as described [here](https://docs.anthropic.com/claude/docs/constructing-a-prompt#system-prompt-optional)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------PROMPT----------\n",
      "Who directed the Avatar film series?\n",
      "---------RESPONSE---------\n",
      "James Cameron directed the Avatar film series.\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"Who directed the Avatar film series?\"\n",
    "\n",
    "print(\"----------PROMPT----------\")\n",
    "print(prompt)\n",
    "print(\"---------RESPONSE---------\")\n",
    "print(llm_call(model=\"gpt-4\", prompt=prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------SYSTEM PROMPT-------\n",
      "You are a helpful chat assistant. Always follow these rules:\n",
      "1. Speak in the style of a voiceover for an action movie.\n",
      "2. Always end sentences with a joke. \n",
      "3. Use the words \"If you please\" in every response.\n",
      "----------PROMPT----------\n",
      "Who directed the Avatar film series?\n",
      "---------RESPONSE---------\n",
      "In the swirling mists of Hollywood, one man stood tall, his vision piercing the veil of cinematic cliches. With a determined heart and a firm hand, he embarked on the journey, ruling with an iron gaze. That man, ladies and gentlemen, is none other than James Cameron, the mastermind behind the Avatar Film Series! He's been sinking the competition since Titanic. Now, that's a deep joke, if you please!\n"
     ]
    }
   ],
   "source": [
    "system_prompt = \"\"\"You are a helpful chat assistant. Always follow these rules:\n",
    "1. Speak in the style of a voiceover for an action movie.\n",
    "2. Always end sentences with a joke. \n",
    "3. Use the words \"If you please\" in every response.\"\"\"\n",
    "\n",
    "prompt = f\"Who directed the Avatar film series?\"\n",
    "\n",
    "print(\"------SYSTEM PROMPT-------\")\n",
    "print(system_prompt)\n",
    "print(\"----------PROMPT----------\")\n",
    "print(prompt)\n",
    "print(\"---------RESPONSE---------\")\n",
    "print(llm_call(model=\"gpt-4\", prompt=prompt, system_prompt=system_prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory\n",
    "\n",
    "In a conversational system, we would expect the model to be able to remember previous parts of the conversation and reference them during new responses. This requires the LLM to hold this information somewhere in memory. \n",
    "\n",
    "The simplest way to do this is to literally just store the entire conversation in a list and pass it along with each model call. \n",
    "\n",
    "Note: Only Chat models use this concept. To replicate the same with an instruction fine-tuned model, just add everything into the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------SYSTEM PROMPT-------\n",
      "You are a helpful chat assistant.\n",
      "----------PROMPT----------\n",
      "Who played Saruman in the Lord of the Rings trilogy?\n",
      "---------RESPONSE---------\n",
      "Saruman in the Lord of the Rings trilogy was played by Sir Christopher Lee.\n"
     ]
    }
   ],
   "source": [
    "system_prompt = \"You are a helpful chat assistant.\"\n",
    "prompt = \"Who played Saruman in the Lord of the Rings trilogy?\"\n",
    "\n",
    "print(\"------SYSTEM PROMPT-------\")\n",
    "print(system_prompt)\n",
    "print(\"----------PROMPT----------\")\n",
    "print(prompt)\n",
    "print(\"---------RESPONSE---------\")\n",
    "result, chat_history = llm_call(model=\"gpt-4\", prompt=prompt, system_prompt=system_prompt, return_chat_history=True)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's continue the conversation. First, let's look at what the chat history looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system', 'content': 'You are a helpful chat assistant.'},\n",
       " {'role': 'user',\n",
       "  'content': 'Who played Saruman in the Lord of the Rings trilogy?'},\n",
       " {'role': 'assistant',\n",
       "  'content': 'Saruman in the Lord of the Rings trilogy was played by Sir Christopher Lee.'}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------SYSTEM PROMPT-------\n",
      "You are a helpful chat assistant.\n",
      "----------PROMPT----------\n",
      "What are some facts about him?\n",
      "---------RESPONSE---------\n",
      "Sir Christopher Lee was a British actor with a rich and varied career in film, television, and music, spanning over seven decades. Here are some facts about him:\n",
      "\n",
      "1. He was born on May 27, 1922, in Belgravia, London, England, and died on June 7, 2015.\n",
      "\n",
      "2. Lee is best known for his roles in horror films produced by Hammer Studios from the late 1950s. He played iconic roles like Count Dracula and Frankenstein's monster.\n",
      "\n",
      "3. Apart from working in horror movies, he's also famous for his roles in The Lord of the Rings and The Hobbit trilogies as Saruman, and in the Star Wars prequel trilogy as Count Dooku.\n",
      "\n",
      "4. He was knighted for services to drama and charity in 2009 and received the BAFTA Fellowship in 2011.\n",
      "\n",
      "5. Lee was a step-cousin of Ian Fleming, author of the James Bond spy novels. He even played a Bond villain, Francisco Scaramanga, in \"The Man with the Golden Gun.\"\n",
      "\n",
      "6. Aside from his acting career, Lee was also known for his deep, strong voice and released several albums and singles as a heavy metal singer.\n",
      "\n",
      "7. During World War II, Lee joined the Royal Air Force and Intelligence Service and performed many missions all over Europe.\n",
      "\n",
      "8. Lee was multilingual and could speak English, Italian, French, Spanish, German, and some Swedish and Russian.\n",
      "\n",
      "9. He holds the world record for most film acting roles ever - he appeared in more than 275 films.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "system_prompt = \"You are a helpful chat assistant.\"\n",
    "prompt = \"What are some facts about him?\"\n",
    "\n",
    "print(\"------SYSTEM PROMPT-------\")\n",
    "print(system_prompt)\n",
    "print(\"----------PROMPT----------\")\n",
    "print(prompt)\n",
    "print(\"---------RESPONSE---------\")\n",
    "result, chat_history = llm_call(model=\"gpt-4\", prompt=prompt, system_prompt=system_prompt, return_chat_history=True, chat_history=chat_history)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we see that the model uses the context from the previous conversation to remember that \"him\" refers to Sir Christopher Lee and generates some facts about him. Note: These facts may or may not be true and should be taken with a grain of salt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taking memory further\n",
    "\n",
    "In upcoming sessions, we'll take this idea of memory further with the introduction of vector databases and Retrieval Augmented Generation (RAG). We use these systems in scenarios where we have a lot of information to refer to and need a way of finding the most relevant information for the conversation at hand."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai-bootcamp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
