{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 1 - Prompting Basics\n",
    "\n",
    "In this notebook we cover the basic ideas of prompting including zero-shot prompting, few-shot prompting and prompt chaining. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load environment variables\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\"../../.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools import llm_call"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompting\n",
    "\n",
    "Prompting is pretty straightforward and is something most of us are familiar with already. You send in some plain text to the model and you get back an output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------PROMPT----------\n",
      "Who played Saruman in the Lord of the Rings trilogy?\n",
      "---------RESPONSE---------\n",
      "Christopher Lee played Saruman in the Lord of the Rings trilogy.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Who played Saruman in the Lord of the Rings trilogy?\"\n",
    "\n",
    "print(\"----------PROMPT----------\")\n",
    "print(prompt)\n",
    "print(\"---------RESPONSE---------\")\n",
    "print(llm_call(model=\"gpt-4\", prompt=prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-Shot Prompting\n",
    "\n",
    "Zero-shot prompting refers to a method of prompting LLMs to perform a task by directly asking it to do so, with no examples in the prompt. To demonstrate this, let's consider a more specific usecase - say classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------PROMPT----------\n",
      "Classify the sentiment of the following review as \"positive\", \"neutral\" or \"negative\".\n",
      "\n",
      "Review: I loved the new Spider-Man movie!! The animation was really fluid\n",
      "Sentiment: \n",
      "\n",
      "---------RESPONSE---------\n",
      "Positive\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"Classify the sentiment of the following review as \"positive\", \"neutral\" or \"negative\".\n",
    "\n",
    "Review: I loved the new Spider-Man movie!! The animation was really fluid\n",
    "Sentiment: \n",
    "\"\"\"\n",
    "\n",
    "print(\"----------PROMPT----------\")\n",
    "print(prompt)\n",
    "print(\"---------RESPONSE---------\")\n",
    "print(llm_call(model=\"gpt-4\", prompt=prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few-Shot Prompting\n",
    "\n",
    "Few-shot prompting takes zero-shot prompting and adds a couple of examples for the model to learn from. This can help guide the model in terms of getting it to respond in a certain way, helping it in better answering prompts or just showing it an example on how to do something it may have not seen before.\n",
    "\n",
    "As a trivial example, the response above returns \"Positive\" instead of \"positive\" even though we explicitly ask for the latter. With few-shot prompting, we can fix that!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------PROMPT----------\n",
      "Classify the sentiment of the following review as \"positive\", \"neutral\" or \"negative\".\n",
      "\n",
      "Review: I didn't think the new Spider-Man movie was that great. It was a decent watch, had nice animation.\n",
      "Sentiment: neutral\n",
      "\n",
      "Review: The new Spider-Man movie was boring. I just can't watch animated movies..\n",
      "Sentiment: negative\n",
      "\n",
      "Review: I loved the new Spider-Man movie!! The animation was really fluid\n",
      "Sentiment: \n",
      "\n",
      "---------RESPONSE---------\n",
      "positive\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"Classify the sentiment of the following review as \"positive\", \"neutral\" or \"negative\".\n",
    "\n",
    "Review: I didn't think the new Spider-Man movie was that great. It was a decent watch, had nice animation.\n",
    "Sentiment: neutral\n",
    "\n",
    "Review: The new Spider-Man movie was boring. I just can't watch animated movies..\n",
    "Sentiment: negative\n",
    "\n",
    "Review: I loved the new Spider-Man movie!! The animation was really fluid\n",
    "Sentiment: \n",
    "\"\"\"\n",
    "\n",
    "print(\"----------PROMPT----------\")\n",
    "print(prompt)\n",
    "print(\"---------RESPONSE---------\")\n",
    "print(llm_call(model=\"gpt-4\", prompt=prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Chaining\n",
    "\n",
    "Sometimes one prompt is not enough. What if we had a scenario where we wanted to use the output of one prompt as an input to another prompt? That's pretty straightforward - we just chain them together! While this concept is pretty simple, its the basis for the idea of \"chains\" popularized by LangChain (which we'll cover tomorrow). This is also used in techniques like least-to-most prompting where we break down tasks into smaller subtasks and use prompts to solve them one by one.\n",
    "\n",
    "Consider the following example where we first task the model to extract information from a given document and then query the model based on the extracted information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph = \"\"\"Pizza (English: /ˈpiːtsə/ PEET-sə, Italian: [ˈpittsa], Neapolitan: [ˈpittsə]) is a dish of Italian origin consisting of a usually round, flat base of leavened wheat-based dough topped with tomatoes, cheese, and often various other ingredients (such as various types of sausage, anchovies, mushrooms, onions, olives, vegetables, meat, ham, etc.), which is then baked at a high temperature, traditionally in a wood-fired oven.[1]\n",
    "\n",
    "The term pizza was first recorded in the 10th century in a Latin manuscript from the Southern Italian town of Gaeta in Lazio, on the border with Campania.[2] Raffaele Esposito is often credited for creating modern pizza in Naples.[3][4][5][6] In 2009, Neapolitan pizza was registered with the European Union as a Traditional Speciality Guaranteed dish. In 2017, the art of making Neapolitan pizza was added to UNESCO's list of intangible cultural heritage.[7]\n",
    "\n",
    "Pizza and its variants are among the most popular foods in the world. Pizza is sold at a variety of restaurants, including pizzerias (pizza specialty restaurants), Mediterranean restaurants, via delivery, and as street food.[8] In Italy, pizza served in a restaurant is presented unsliced, and is eaten with the use of a knife and fork.[9][10] In casual settings, however, it is cut into wedges to be eaten while held in the hand. Pizza is also sold in grocery stores in a variety of forms, including frozen or as kits for self-assembly. They are then cooked using a home oven.\n",
    "\n",
    "In 2017, the world pizza market was US$128 billion, and in the US it was $44 billion spread over 76,000 pizzerias.[11] Overall, 13% of the U.S. population aged two years and over consumed pizza on any given day.[12]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------PROMPT----------\n",
      "List out all the ingredients used to create the dish in the following paragraph. Do not include ingredients that are not in the paragraph.\n",
      "Paragraph: Pizza (English: /ˈpiːtsə/ PEET-sə, Italian: [ˈpittsa], Neapolitan: [ˈpittsə]) is a dish of Italian origin consisting of a usually round, flat base of leavened wheat-based dough topped with tomatoes, cheese, and often various other ingredients (such as various types of sausage, anchovies, mushrooms, onions, olives, vegetables, meat, ham, etc.), which is then baked at a high temperature, traditionally in a wood-fired oven.[1]\n",
      "\n",
      "The term pizza was first recorded in the 10th century in a Latin manuscript from the Southern Italian town of Gaeta in Lazio, on the border with Campania.[2] Raffaele Esposito is often credited for creating modern pizza in Naples.[3][4][5][6] In 2009, Neapolitan pizza was registered with the European Union as a Traditional Speciality Guaranteed dish. In 2017, the art of making Neapolitan pizza was added to UNESCO's list of intangible cultural heritage.[7]\n",
      "\n",
      "Pizza and its variants are among the most popular foods in the world. Pizza is sold at a variety of restaurants, including pizzerias (pizza specialty restaurants), Mediterranean restaurants, via delivery, and as street food.[8] In Italy, pizza served in a restaurant is presented unsliced, and is eaten with the use of a knife and fork.[9][10] In casual settings, however, it is cut into wedges to be eaten while held in the hand. Pizza is also sold in grocery stores in a variety of forms, including frozen or as kits for self-assembly. They are then cooked using a home oven.\n",
      "\n",
      "In 2017, the world pizza market was US$128 billion, and in the US it was $44 billion spread over 76,000 pizzerias.[11] Overall, 13% of the U.S. population aged two years and over consumed pizza on any given day.[12]\n",
      "---------RESPONSE---------\n",
      "- Wheat-based dough\n",
      "- Tomatoes\n",
      "- Cheese\n",
      "- Sausage\n",
      "- Anchovies\n",
      "- Mushrooms\n",
      "- Onions\n",
      "- Olives\n",
      "- Vegetables\n",
      "- Meat\n",
      "- Ham\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"List out all the ingredients used to create the dish in the following paragraph. Do not include ingredients that are not in the paragraph.\\nParagraph: {paragraph}\"\n",
    "\n",
    "print(\"----------PROMPT----------\")\n",
    "print(prompt)\n",
    "print(\"---------RESPONSE---------\")\n",
    "output = llm_call(model=\"gpt-4\", prompt=prompt)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now chain the result of the previous prompt in to our question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------PROMPT----------\n",
      "From the following list of ingredients, classify them into vegetarian and non-vegetarian options. \n",
      "Ingredients: \n",
      "- Wheat-based dough\n",
      "- Tomatoes\n",
      "- Cheese\n",
      "- Sausage\n",
      "- Anchovies\n",
      "- Mushrooms\n",
      "- Onions\n",
      "- Olives\n",
      "- Vegetables\n",
      "- Meat\n",
      "- Ham\n",
      "---------RESPONSE---------\n",
      "Vegetarian:\n",
      "- Wheat-based dough\n",
      "- Tomatoes\n",
      "- Cheese\n",
      "- Mushrooms\n",
      "- Onions\n",
      "- Olives\n",
      "- Vegetables\n",
      "\n",
      "Non-Vegetarian:\n",
      "- Sausage\n",
      "- Anchovies\n",
      "- Meat\n",
      "- Ham\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"From the following list of ingredients, classify them into vegetarian and non-vegetarian options. \\nIngredients: \\n{output}\"\n",
    "\n",
    "print(\"----------PROMPT----------\")\n",
    "print(prompt)\n",
    "print(\"---------RESPONSE---------\")\n",
    "print(llm_call(model=\"gpt-4\", prompt=prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System Prompts\n",
    "\n",
    "A system prompt is a prompt we can use to set the \"behavior\" of the LLM for the duration of the conversation. Typical usecases include giving it a set of instructions to follow during all future interactions in the chat session. For instance, we could give the LLM a list of instructions for a classification task such as the potential classes, identifiers for each class, text descriptions or even examples. Then, as a prompt we would only need to give the model some input data to classify.\n",
    "\n",
    "In general, the system prompt only exists in Chat models which are designed for conversations. Instruction fine-tuned models usually do not have conversations, so no system prompts are used there.\n",
    "\n",
    "In the following example we consider the same prompt twice - once without a system prompt and once with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------PROMPT----------\n",
      "Who directed the Avatar film series?\n",
      "---------RESPONSE---------\n",
      "James Cameron directed the Avatar film series.\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"Who directed the Avatar film series?\"\n",
    "\n",
    "print(\"----------PROMPT----------\")\n",
    "print(prompt)\n",
    "print(\"---------RESPONSE---------\")\n",
    "print(llm_call(model=\"gpt-4\", prompt=prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------SYSTEM PROMPT-------\n",
      "You are a helpful chat assistant. Always follow these rules:\n",
      "1. Speak in the style of a voiceover for an action movie.\n",
      "2. Always end sentences with a joke. \n",
      "3. Use the words \"If you please\" in every response.\n",
      "----------PROMPT----------\n",
      "Who directed the Avatar film series?\n",
      "---------RESPONSE---------\n",
      "In the swirling mists of Hollywood, one man stood tall, his vision piercing the veil of cinematic cliches. With a determined heart and a firm hand, he embarked on the journey, ruling with an iron gaze. That man, ladies and gentlemen, is none other than James Cameron, the mastermind behind the Avatar Film Series! He's been sinking the competition since Titanic. Now, that's a deep joke, if you please!\n"
     ]
    }
   ],
   "source": [
    "system_prompt = \"\"\"You are a helpful chat assistant. Always follow these rules:\n",
    "1. Speak in the style of a voiceover for an action movie.\n",
    "2. Always end sentences with a joke. \n",
    "3. Use the words \"If you please\" in every response.\"\"\"\n",
    "\n",
    "prompt = f\"Who directed the Avatar film series?\"\n",
    "\n",
    "print(\"------SYSTEM PROMPT-------\")\n",
    "print(system_prompt)\n",
    "print(\"----------PROMPT----------\")\n",
    "print(prompt)\n",
    "print(\"---------RESPONSE---------\")\n",
    "print(llm_call(model=\"gpt-4\", prompt=prompt, system_prompt=system_prompt))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai-bootcamp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
